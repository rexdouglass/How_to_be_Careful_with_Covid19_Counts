--- 
title: "How to be Careful with Covid-19 Counts: A Practical Guide to Working with Pandemic Panel Data"
author: "Rex W. Douglass"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: ""
---

    

```{r,eval=T, warning=FALSE,message=FALSE,error=FALSE, results='hide' , include=FALSE}
knitr::opts_chunk$set( out.width='100%', fig.align='center', echo=F, warning=FALSE,message=FALSE,error=FALSE, results='hide', cache=T  ) 

#This is a _sample_ book written in **Markdown**. You can use anything that Pandoc's Markdown supports, e.g., a math equation $a^2 + b^2 = c^2$.

#The **bookdown** package can be installed from CRAN or Github:

#Notes to self
#It's not like R notebook you have the knit every time you want to update html, you can't just save
#You don't have to rerun the code or rebuild the whole book though.

```

```{r eval=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide'}
#install.packages("bookdown")
# or the development version
# devtools::install_github("rstudio/bookdown")
```

<!-- Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading `#`. -->

<!-- To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): <https://yihui.name/tinytex/>.   -->

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->


```{r,warning=FALSE,message=FALSE,error=FALSE, results='hide' , include=FALSE}
knitr::opts_chunk$set( out.width='100%', fig.asp=NULL, fig.align='center', echo=F, warning=FALSE,message=FALSE,error=FALSE, results='hide', cache=T  ) 

#Notes to self
#It's not like R notebook you have the knit every time you want to update html, you can't just save
#You don't have to rerun the code or rebuild the whole book though.

```

```{r}
#Library Loads
library(scales) 
library(tidyverse)
# stable version on CRAN
#install.packages("bookdown")
# or development version on GitHub
# devtools::install_github('rstudio/bookdown')

```


```{r}

library(tsibble)

lhs_long <- readRDS("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/lhs_long.Rds")
dim(lhs_long) #779818

####crap, some package I loaded breaks dplyr badly. The only one I can think of is UpSetR.

lhs_long_clean <- lhs_long %>% as.data.frame() %>% ungroup() %>% 
                  #filter(wikidata_id=="Q30" ) %>% #& dataset=="yahoo"
                  mutate(dataset_gid_geonameid_wikidata_id= paste(dataset,gid,geonameid, wikidata_id, sep="_")  ) %>%
                  mutate(gid_geonameid_wikidata_id= paste(gid,geonameid, wikidata_id, sep="_")  ) %>%

                  dplyr::select(dataset_gid_geonameid_wikidata_id, gid_geonameid_wikidata_id, dataset, gid,geonameid, wikidata_id ,date_asdate, confirmed, deaths, tested_people, tested_samples) %>%
  
                  group_by(dataset_gid_geonameid_wikidata_id ,date_asdate) %>% #if the same source has multiple values on the same thing on the same day, or different values across unmerged obs, then take the max
                    summarise_all(max, na.rm=T) %>% 
                  ungroup() %>%
  
                  mutate_if(is.numeric, list(~na_if(., -Inf))) %>%
                  mutate_if(is.numeric, list(~na_if(., Inf))) %>%
  
                  filter(!is.na(date_asdate)) %>%
                  
                  #First drop anything that's negative. Shouldn't be any negatives.
                  filter(is.na(confirmed) | confirmed>=0) %>%
                  filter(is.na(deaths) | deaths>=0) %>%
                  filter(is.na(tested_people) | tested_people>=0) %>%
                  filter(is.na(tested_samples) | tested_samples>=0) %>%
                  
                  #Then set 0s to NA, we don't trust NAs
                  mutate(confirmed=ifelse(confirmed==0, NA, confirmed)) %>%
                  mutate(deaths=ifelse(deaths==0, NA, deaths)) %>%
                  mutate(tested_people=ifelse(tested_people==0, NA, tested_people)) %>%
                  mutate(tested_samples=ifelse(tested_samples==0, NA, tested_samples)) %>%
                
                  #Then check first differences and drop anything that doesn't weakly increase monotonically
                  dplyr::group_by(  dataset_gid_geonameid_wikidata_id ) %>%  #it's not grouping correctly all of a sudden?
                    dplyr::arrange(date_asdate) %>%
                    mutate(confirmed_cummax =    confirmed %>% replace_na(0) %>% cummax(),
                           deaths_cummax    =    deaths %>% replace_na(0) %>% cummax(),
                           tested_people_cummax= tested_people %>% replace_na(0) %>% cummax(), 
                           tested_samples_cummax=tested_samples %>% replace_na(0) %>% cummax()
                           ) %>%
                  ungroup() %>%  
  
                  #About 4k of these observations show a decrease from one time step to the next which suggests either an original error or a joining error
                  #We're going to straight drop those observations as a cleaning step
                  #this isn't enough because if you have consecutive mistakes it'll show a positive fd even if it's not good enough
                  filter( (is.na(confirmed) | confirmed>=confirmed_cummax) &  #never allow for a reversal
                          (is.na(deaths) | deaths>=deaths_cummax) &        #never allow for a reversal
                          (is.na(tested_people) | tested_people>=tested_people_cummax) &  #never allow for a reversal
                          (is.na(tested_samples) | tested_samples>=tested_samples_cummax)  #never allow for a reversal
                          ) %>%
  #we're going to go one step further and require either confirmed or tested to be strictly higher
  #In words, during an episode we don't believe reports mean "no new cases" just no new good reporting
  #metabiota is the worst offender here so restricting it to just that
  filter( !(
            dataset=="metabiota" & #is metabiota
            (!is.na(confirmed) &   #with a non missing confirmed
              confirmed<=confirmed_cummax ) #that is equal to or less than the cumulative max until then.
            ) #reject these
          ) %>% #metabiota is really problematic 90% of what we're doing is trying to account for it
  
  #Also reject any where deaths are greater than confirmed
  filter(is.na(confirmed) | is.na(deaths) | confirmed>=deaths) #%>%
  
  #also reject any where confirmed doesn't vary at all at that location
  #I should do this but it's throwing weird warnings
  #group_by( gid_geonameid_wikidata_id) %>% #
  #  filter( confirmed %>% unique() %>% length() > 1 ) %>%
  #ungroup()

dim(lhs_long_clean) #675,696

saveRDS(lhs_long_clean,
        "/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/lhs_long_cleand.Rds")

```


```{r}
#Data Loads
lhs_long <- readRDS("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/lhs_long.Rds")
#Data loads
lhs_long_clean <- readRDS( "/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/lhs_long_cleand.Rds")


```

# Introduction {#intro}

How should we interpret the endless stream of figures and maps of COVID-19 produced by health departments and organizations around the world? For better or worse, we primarily experience large complicated events through counts- How many are dead?; How many are sick?; How many tests did we perform? These are universal questions, immediately accessible to both the producers of information like doctors and scientists and consumers of information like policy makers and citizens. For anyone who regularly works with the answers to those questions, the actual data, you know that every one of those simple numbers in a cell needs a big asterisks pointing to a long footnote explaining all of the problems in developing and using that number. This book is that footnote for COVID-19 counts. It is intended as a practical guide for using COVID-19 data, and all of the regularities and subtle gotchas you can expect to find.

This guide is built around a new resource developed at the Machine Learning for Social Science Lab called the Global Census of Covid-19 Counts (GC3). This is a single normalized and georeferenced aggregation of all of the other public aggregations of COVID-19 counts data available. We are currently aggregating `r lhs_long_clean$dataset %>% unique() %>% length()` databases, who are in turn scraping and aggregating over ten thousand sources like public statements, news reports, and individual health department websites. <!-- Using an existing system we have in development, the Named Entities for Social Science knowledge graph, we were able to convert all of the disparate natural language geographic names to a single uniform key index, which can be taken directly to other standard databases of demographic and medical information. --> Only by mosaicing all of these different resource together (`r lhs_long %>% nrow() %>% comma()` observations and growing), are we able to finally provide full temporal coverage over the entire COVID-19 pandemic, and full spatial coverage over all countries in the world and in most places states/provinces as well. We are now able to track counts of confirmed cases and deaths in `r lhs_long_clean %>% pull(gid_geonameid_wikidata_id) %>% unique() %>% length() %>% comma()` locations, and number of tests performed in `r lhs_long_clean %>% filter(!is.na(tested_people) | !is.na(tested_samples)) %>% pull(gid_geonameid_wikidata_id) %>% unique() %>% length() %>% comma()` locations.

This book is a deep dive into what problems and opportunities you can expect to find in these data. It is organized in order from simpler issues of data acquisition and aggregation, to more complicated questions of bias and latent true measurement.

## Definitions and Unit of Analysis

## Summary of the Findings (TLDR)


```{r nice-fig, eval=F, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}

#You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods).
#Figures and tables with captions will be placed in `figure` and `table` environments, respectively.

par(mar = c(4, 4, .1, .1))
plot(pressure, type = 'b', pch = 19)
```


```{r nice-tab, eval=F , tidy=FALSE}
#Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab).

knitr::kable(
  head(iris, 20), caption = 'Here is a nice table!',
  booktabs = TRUE
)
```

You can write citations, too. For example, we are using the **bookdown** package [@R-bookdown] in this sample book, which was built on top of R Markdown and **knitr** [@xie2015].

<!--chapter:end:01-intro.Rmd-->


```{r,warning=FALSE,message=FALSE,error=FALSE, results='hide' , include=FALSE}
knitr::opts_chunk$set( out.width='100%', fig.asp=NULL, fig.align='center', echo=F, warning=FALSE,message=FALSE,error=FALSE, results='hide', cache=T  ) 

#Notes to self
#It's not like R notebook you have the knit every time you want to update html, you can't just save
#You don't have to rerun the code or rebuild the whole book though.

```

```{r}
#Library Loads
library(scales) 
library(tidyverse)
# stable version on CRAN
#install.packages("bookdown")
# or development version on GitHub
# devtools::install_github('rstudio/bookdown')
#libraries
library(lubridate)
library(tidyverse)

#devtools::install_github("ropensci/USAboundaries")
#devtools::install_github("ropensci/USAboundariesData")
library(USAboundaries) ; #install.packages('USAboundaries')
data(state_codes)

library(tidyverse)
library(scales)
library(gghighlight)
library(lubridate)
library(R0)  # consider moving all library commands to top -- this one was in a loop below

library(WikidataR)
library(countrycode)

library(usmap) ; # install.packages('usmap')
data(statepop)
#devtools::install_github("ropensci/USAboundaries")
#devtools::install_github("ropensci/USAboundariesData")
library(USAboundaries) ; #install.packages('USAboundaries')
data(state_codes)

library(tidyverse)
library(sf)

library(jsonlite)

#This is too slow it's downloading each
library(GADMTools)
library(strucchange) ; #install.packages('strucchange')
library(tsibble)

library(patchwork)
library(DT)
```


```{r}
#Data Loads
lhs_long <- readRDS("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/lhs_long.Rds")
#Data loads
lhs_long_clean <- readRDS( "/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/lhs_long_cleand.Rds")

#dim(lhs_long) #187,305
#lhs_long_clean$dataset

```


# Global COVID-19 Count Data

## What data are available?

The Global Census of Covid-19 Counts (GC3) currently aggregates `r lhs_long_clean$dataset %>% unique() %>% length()` databases. The databases vary drastically in size, scope, collection method, and purpose. On the small end are github repositories built around collecting a single country's published statistics, often available in an unstructured form on a government website in a native language. Others are official government statistics reported directly to and compiled by international organizations, like the World Health Organization (WHO) or the European Centre for Disease Prevention and Control. Some are news organizations that collect and compile official government statistics, like the New York Times and Reuters. Nonprofits like the Covidtracking Project compile records on specific issues like testing. Wikipedia provides an interface for a massive army of volunteers to enter in statistics into tabular formats that can later be scraped.  The largest and most comprehensive scraping effort is the Corona Data Scraper from the Covid Atlas which only consumes sources directly from government and health organizations (excluding news and wikipeda). These all in turn are then ingested by larger aggregation projects. Johns Hopkins University Center for Systems Science and Engineering (JHU-CSSE) is the most widely used aggregator by downstream projects. Both Microsoft's Bing research Unit and Yahoo! have in turn recently made available their knowledge graph coverage of Covid-19 counts.

Their names, links, and cleaned observation counts appear in the table below.

```{r}
print("Number of observations")
dim(lhs_long_clean)

print("Number of locations")
lhs_long_clean$wikidata_id %>% unique() %>% length() #3,373

print("Number of Days")
lhs_long_clean$date_asdate %>% unique() %>% length() #113

library(DT) #https://rstudio.github.io/DT/

#library(gt)
#lhs_long_clean %>% dplyr::count(dataset) %>% arrange(-n) %>% gt() %>% 
# tab_header(
#    title = md("Location-Days by Dataset")#,
#    #subtitle = "Number of Days with Data by Dataset"
#  ) %>%
#  fmt_missing( columns=everything(), rows=NULL,missing_text = 0)

```

```{r, results="show"}
lhs_long_clean %>% dplyr::count(dataset) %>% arrange(-n) %>% DT::datatable(options = list(pageLength = 20, autoWidth = TRUE))
```

```{r}

#There's a miscode of New York City and New York state that creates an issue
lhs_long_clean_confirmed_wide <-
                              lhs_long_clean %>%
                              filter(!is.na(wikidata_id)) %>% #the NA_NA_NA obs were screwing with us
                              dplyr::select(dataset, gid_geonameid_wikidata_id, date_asdate, confirmed) %>%
                              filter(!is.na(gid_geonameid_wikidata_id) & !is.na(date_asdate)) %>%
                              filter(!is.na(confirmed) & confirmed>0 ) %>%
                              group_by(dataset, gid_geonameid_wikidata_id, date_asdate) %>%
                                dplyr::summarise(confirmed= max(confirmed, na.rm=T)) %>%
                              ungroup() %>%
                              pivot_wider(names_from = dataset, values_from = confirmed ) %>%
                              mutate_if(is.numeric, list(~na_if(abs(.), Inf))) %>% distinct()
dim(lhs_long_clean_confirmed_wide) #154,499 154k location days

temp <- lhs_long_clean_confirmed_wide  %>% mutate_if(is.numeric, replace_na, 0) %>% mutate_if(is.numeric, ~  as.numeric(.>0 )) #%>% dplyr::select(-gid_geonameid_wikidata_id, -date_asdate)
```

The unit of observation in our data is the location-day. The Upset plot in Figure \@ref(fig:upset1) shows the number of unique location-day observations provided by each database, as well as for those available on in two datasets. Which database will provide the most unique information is difficult to tell apriori. The most unique contributions come from the Corona Data Scraper Project, which might be anticipated by their overall size. The second most unique observations however comes from Wikipedia which is surprising because our treatment of it is currently ad hoc and it should already be ingested by other sources. It goes to show that no single source, or even no small combination of sources, is sufficient to provide full temporal and spatial coverage over even this relatively brief period of the Covid-19 pandemic.

```{r upset1}
library(UpSetR) #install.packages('UpSetR')
upset(temp %>% as.data.frame(), nsets = 25, nintersects = 30, mb.ratio = c(0.4, 0.6),
      order.by = c("freq", "degree"), decreasing = c(T,FALSE))
detach(package:UpSetR, unload=TRUE) #I currently suspect this of breaking dplyr in really subtle quite ways that are noooot good.
```


## What is their geographic coverage?

```{r}


lhs_long_place_sources <- lhs_long  %>% dplyr::select(gid,   geonameid, wikidata_id, dataset) %>% 
                           group_by(gid,  geonameid, wikidata_id) %>% dplyr::count(dataset) %>%
                           group_by(gid,  geonameid, wikidata_id) %>%
                           dplyr::summarise(datasets_n=n()) 

lhs_long_place_sources_testing <- lhs_long  %>%
                                  dplyr::select(gid,  geonameid, wikidata_id, date_asdate, confirmed, tested_people, tested_samples) %>%
                                  group_by(gid,  geonameid, wikidata_id, date_asdate) %>% 
                                  mutate_if(is.numeric, max, na.rm=T) %>% #this takes a max across all datasets for that day
                                  filter(!duplicated(date_asdate)) %>%
                                  ungroup() %>%
                                  mutate_if(is.numeric, list(~na_if(., -Inf))) %>%
                                  mutate_if(is.numeric, list(~na_if(., Inf))) %>%
                                  mutate(percent_of_days_without_testing=!is.na(date_asdate) & (is.na(tested_people) & is.na(tested_samples)  )) %>%
                                  
                                  group_by(gid,  geonameid, wikidata_id) %>% 
                                  dplyr::summarise(
                                            percent_of_days_without_testing=sum(percent_of_days_without_testing, na.rm=T)/n()
                                            ) %>%
                                  mutate(percent_of_days_with_testing= 1-percent_of_days_without_testing) 

```

### Country Level Data Availability


```{r}

#gadm36 = st_read("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESSgadm/data_in/gadm36_gpkg/gadm36.gpkg")
#st_layers("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/gadm36_bycountry/gadm36_levels_gpkg/gadm36_levels.gpkg")
gadm36_levels_0 = st_read("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/gadm36_bycountry/gadm36_levels_gpkg/gadm36_levels.gpkg", layer="level0")  %>%
                  st_simplify(preserveTopology = FALSE, dTolerance =0.1) #  0.025 this is supposedly broken up by 6 levels and so should have u.s. 
#plot(gadm36_levels_0)
#dim(gadm36_levels_0_sf$sf)
#gadm_plot(gadm36_levels_0_sf)

```

```{r}

p_datasets_by_country <- gadm36_levels_0 %>% 
      left_join(lhs_long_place_sources %>% dplyr::select(gid=gid, datasets_n) %>%  left_join( gadm36_levels_0 %>% as.data.frame() %>% dplyr::select(gid=GID_0, NAME_0)  %>% distinct()  )
                ) %>%
      #replace_na(list(datasets_n = 0)) %>% 
      ggplot() + geom_sf(aes(fill = datasets_n)) +
      scale_fill_gradient(name= "# Databases", low="red", high="green") +
      theme_bw() + ggtitle("Number of Databases with Coverage of each Country")


p0_testing <- gadm36_levels_0 %>% 
  left_join(lhs_long_place_sources_testing %>% dplyr::select(gid=gid, percent_of_days_with_testing) %>%  left_join( gadm36_levels_0 %>% as.data.frame() %>% dplyr::select(gid=GID_0, NAME_0)  %>% distinct()  )
  ) %>%
  #replace_na(list(datasets_n = 0)) %>% 
  ggplot() + geom_sf(aes(fill = percent_of_days_with_testing)) +
  scale_fill_gradient(name= "Percent", low="red", high="green") +
  theme_bw() + ggtitle("Percent of Days with Testing Counts")

```
Despite this major effort by data producers, collectors, and aggregators, there is still major geographic variation in availability across countries. Most notably in availability of counts on number of tests performed, particularly in Central Africa.


```{r, nice-fig2, fig.cap='Data coverage by country.'}
p_datasets_by_country / p0_testing
```


### State/Province Level Data Availability

Disparities in coverage across countries is most dramatic at the subnational level.

```{r}
gadm36_levels_1 = st_read("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/gadm36_bycountry/gadm36_levels_gpkg/gadm36_levels.gpkg", layer="level1")  %>%
  st_simplify(preserveTopology = FALSE, dTolerance =0.1) #  0.025 this is supposedly broken up by 6 levels and so should have u.s. 
```

```{r}
p_datasets_by_state <- gadm36_levels_1 %>% 
                        left_join(lhs_long_place_sources %>% dplyr::select(gid=gid, datasets_n) %>%  left_join( gadm36_levels_1 %>% as.data.frame() %>% dplyr::select(gid=GID_1, NAME_1) %>% distinct()  )
                        ) %>%
                        #replace_na(list(datasets_n = 0)) %>% 
                        ggplot() + geom_sf(aes(fill = datasets_n)) +
                        scale_fill_gradient(name= "# Databases", low="red", high="green") +
                        theme_bw()  + ggtitle("Number of Databases with Coverage of each State/Province")
```


```{r}
p1_testing <- gadm36_levels_1 %>% 
  left_join(lhs_long_place_sources_testing %>% dplyr::select(gid=gid, percent_of_days_with_testing) %>%  left_join( gadm36_levels_1 %>% as.data.frame() %>% dplyr::select(gid=GID_1, NAME_1) %>% distinct()  )
  ) %>%
  #replace_na(list(datasets_n = 0)) %>% 
  ggplot() + geom_sf(aes(fill = percent_of_days_with_testing)) +
  scale_fill_gradient(name= "Percent", low="red", high="green") +
  theme_bw()  + ggtitle("Percent of Days with Testing Counts")
```

```{r, nice-fig3, fig.cap='Data coverage by State/Province'}
p_datasets_by_state/p1_testing
```




### County District Level Data Availability

```{r}
gadm36_levels_2 = st_read("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/gadm36_bycountry/gadm36_levels_gpkg/gadm36_levels.gpkg", layer="level2")  %>%
  st_simplify(preserveTopology = FALSE, dTolerance = 0.001) #  0.025 this is supposedly broken up by 6 levels and so should have u.s. I have to keep shrinking it so misisng goes to zero 
```
This takes a long time to run so we're disabling it until the end
```{r, eval=F}
p2 <- gadm36_levels_2 %>% 
  left_join(lhs_long_place_sources %>% dplyr::select(gid=gid, datasets_n) %>%  left_join( gadm36_levels_2 %>% as.data.frame() %>% dplyr::select(gid=GID_2, NAME_2) %>% distinct()  )
  ) %>%
  #replace_na(list(datasets_n = 0)) %>% 
  ggplot() + geom_sf(aes(fill = datasets_n),lwd = 0) +
  scale_fill_gradient(name= "# Databases", low="blue", high="red") +
  theme_bw() + ggtitle("Number of Databases with Coverage of each County/District")
```

This takes a long time to run so we're disabling it until the end

```{r, nice-fig4, fig.cap='Data coverage by County/District', eval=F}
p2
```



## What is their temporal coverage?

### Comparisons

Figures x,y,z illustrate the problem of data coverage for 3 countries, China, Italy, and the U.S.

```{r }
#China
#"Q148"
test <- lhs_long_clean %>%
  mutate(confirmed_log = log(confirmed)) %>% 
  mutate(deaths_log = log(deaths)) %>% 
  mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>% 
  
  filter(!is.na(confirmed_log) ) %>%
  filter(wikidata_id=="Q148") %>% 
  arrange(date_asdate) #>%

p_over_time_by_source_china <- test %>%
                              ggplot() + 
                              geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
                              geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
                              geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
                              theme_bw() +
                              facet_wrap(~dataset   ) +
                              ylab("Cumulative Count (log)") + xlab("Date") +
                              ggtitle("China Q148 Counts") 

```

China outright refuses to release daily counts of testing. Only three databases document the beginning of the outbreak, the ECDC, the WHO, and Yahoo. On April 17, China [changed its reporting](http://en.nhc.gov.cn/2020-04/17/c_79285.htm) which added 1,290 more deaths for Wuhan city only. The change is not retrospective, it shows up only a sharp discontinuity across multiple datasets. 

```{r p_over_time_by_source_china1}
p_over_time_by_source_china
```


Italy's coverage across datasets is fairly good and uniform, though there are breaks in coverage of testing for some datasets as well as variation in when each dataset starts tracking testing.

```{r}
#Italy
#"Q38"
test <- lhs_long_clean %>%
  mutate(confirmed_log = log(confirmed)) %>% 
  mutate(deaths_log = log(deaths)) %>% 
  mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>% 
  
  filter(!is.na(confirmed_log) ) %>%
  filter(wikidata_id=="Q38") %>% 
  arrange(date_asdate) #%>%
#mutate( confirmed_log_y_hat_percent_change = breakpoints(formula=confirmed_log_y_hat_percent_change ~ 1, data=. ) %>% fitted.values() )

p_over_time_by_source_italy <- test %>%
  ggplot() + 
  geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
  geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
  geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
  theme_bw() +
  ylab("Cumulative Count (log)") + xlab("Date") +
  ggtitle("Italy Q38 Counts") + 
  facet_wrap(~dataset   )

```

```{r p_over_time_by_source_italy1}
p_over_time_by_source_italy
```


The U.S. has a great deal of coverage, but also a great deal of disagreement in that coverage. There is a stair step pattern in confirmed and deaths for Bing, WHO, and Wikipedia. In others reporting from day to day looks more continuous. There is also a change in reporting in late February that shows us a sharp vertical discontinuity across most datasets, though the size of the jump varies. There is also less temporal coverage of testing than is available from the caronavirus tracking project at the state level. Why those state level estiamtes aren't totaled and available at the national level is a question.

```{r}
#U.S.
#"Q30"
test <- lhs_long_clean %>%
  mutate(confirmed_log = log(confirmed)) %>% 
  mutate(deaths_log = log(deaths)) %>% 
  mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>% 
  
  filter(!is.na(confirmed_log) ) %>%
  filter(wikidata_id=="Q30") %>% 
  arrange(date_asdate)

p_over_time_by_source_us <- test %>%
  ggplot() + 
  geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
  geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
  geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
  theme_bw() +
  facet_wrap(~dataset   ) +
  ylab("Cumulative Count (log)") + xlab("Date") +
  ggtitle("United States Q30 Counts")


```

```{r p_over_time_by_source_us1}
p_over_time_by_source_us
```


New York has been the most heavily hit by COVID-19 in the U.S. Two sources, CornaDataScraper and the Covid Tracking Project have coverage over nearly the entire period. However, only one shows a sharp discontinuity in testing around March 10th. Digging into that disagreement more, the CTP rates New York's data release a B quality, coming from snapshots of press conferences and then switching to screenshots of New York's "Department of Health Covid-19 Tracker" website.

```{r}
#New York State
#"Q30"
test <- lhs_long_clean %>%
  mutate(confirmed_log = log(confirmed)) %>% 
  mutate(deaths_log = log(deaths)) %>% 
  mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>% 
  
  filter(!is.na(confirmed_log) ) %>%
  filter(wikidata_id=="Q1384") %>% 
  arrange(date_asdate) 

p_over_time_by_source_new_york_state <- test %>%
                                        ggplot() + 
                                        geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
                                        geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
                                        geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
                                        theme_bw() +
                                        facet_wrap(~dataset   ) +
                                        ylab("Cumulative Count (log)") + xlab("Date") +
                                        ggtitle("New York State Q1384 Counts") 


```

```{r p_over_time_by_source_new_york_state1}
p_over_time_by_source_new_york_state
```


```{r}
#Bronx County (Q855974)
test <- lhs_long_clean %>%
        mutate(confirmed_log = log(confirmed)) %>% 
        mutate(deaths_log = log(deaths)) %>% 
        mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>% 
        filter(!is.na(confirmed_log) ) %>%
        filter(wikidata_id=="Q855974") %>% 
        arrange(date_asdate) 

p_over_time_by_source_Bronx <- test %>%
                            ggplot() + 
                            geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
                            geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
                            geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
                            theme_bw() +
                            facet_wrap(~dataset   ) +
                            ylab("Cumulative Count (log)") + xlab("Date") +
                            ggtitle("Bronx County (Q855974) Counts")

#Brooklyn (Q18419)
test <- lhs_long_clean %>%
        mutate(confirmed_log = log(confirmed)) %>% 
        mutate(deaths_log = log(deaths)) %>% 
        mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>% 
        filter(!is.na(confirmed_log) ) %>%
        filter(wikidata_id=="Q18419") %>% 
        arrange(date_asdate) 

p_over_time_by_source_Brooklyn <- test %>%
                            ggplot() + 
                            geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
                            geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
                            geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
                            theme_bw() +
                            facet_wrap(~dataset   ) +
                            ylab("Cumulative Count (log)") + xlab("Date") +
                            ggtitle("Brooklyn County (Q18419) Counts")


#Queens County, New York Q5142559
test <- lhs_long_clean %>%
        mutate(confirmed_log = log(confirmed)) %>% 
        mutate(deaths_log = log(deaths)) %>% 
        mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>% 
        filter(!is.na(confirmed_log) ) %>%
        filter(wikidata_id=="Q5142559") %>% 
        arrange(date_asdate) 

p_over_time_by_source_Queens <- test %>%
                            ggplot() + 
                            geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
                            geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
                            geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
                            theme_bw() +
                            facet_wrap(~dataset   ) +
                            ylab("Cumulative Count (log)") + xlab("Date") +
                            ggtitle("Queens County (Q18419) Counts")

#Manhattan is New York County Q500416
test <- lhs_long_clean %>%
        mutate(confirmed_log = log(confirmed)) %>% 
        mutate(deaths_log = log(deaths)) %>% 
        mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>% 
        filter(!is.na(confirmed_log) ) %>%
        filter(wikidata_id=="Q500416") %>% 
        arrange(date_asdate) 

p_over_time_by_source_Manhattan <- test %>%
                            ggplot() + 
                            geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
                            geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
                            geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
                            theme_bw() +
                            facet_wrap(~dataset   ) +
                            ylab("Cumulative Count (log)") + xlab("Date") +
                            ggtitle("Manhattan (Q500416) Counts")

#Staten Island is richmond county Q11997784
test <- lhs_long_clean %>%
        mutate(confirmed_log = log(confirmed)) %>% 
        mutate(deaths_log = log(deaths)) %>% 
        mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>% 
        filter(!is.na(confirmed_log) ) %>%
        filter(wikidata_id=="Q11997784") %>% 
        arrange(date_asdate) 

p_over_time_by_source_Staten_Island <- test %>%
                            ggplot() + 
                            geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
                            geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
                            geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
                            theme_bw() +
                            facet_wrap(~dataset   ) +
                            ylab("Cumulative Count (log)") + xlab("Date") +
                            ggtitle("Staten Island  (Q11997784) Counts")

```

```{r}

p_over_time_by_source_Bronx +
#p_over_time_by_source_Brooklyn + #I need to fix brooklyn
p_over_time_by_source_Queens + 
p_over_time_by_source_Manhattan + 
p_over_time_by_source_Staten_Island

```

## Where and How do they Disagree?

```{r}
temp <- lhs_long_clean %>% 
                              filter(!is.na(wikidata_id)) %>% #the NA_NA_NA obs were screwing with us
                              dplyr::select(dataset, gid_geonameid_wikidata_id, date_asdate, confirmed) %>%
                              filter(!is.na(gid_geonameid_wikidata_id) & !is.na(date_asdate)) %>%
                              filter(!is.na(confirmed) & confirmed>0 )

lhs_long_clean_pairwise <- temp %>% left_join(temp, by=c('gid_geonameid_wikidata_id', 'date_asdate'))
dim(lhs_long_clean_pairwise) #3,304,496

lhs_long_clean_pairwise <- lhs_long_clean_pairwise %>% 
                           filter(dataset.x!=dataset.y) %>%
                           mutate(resid_perc= abs((confirmed.x/confirmed.y)-1)  )

dim(lhs_long_clean_pairwise) #2,673,952
lhs_long_clean_pairwise$resid_perc %>% summary() #About within 2% on average but exact usually

```

```{r}
lhs_long_clean_pairwise_date <- lhs_long_clean_pairwise %>%
                                group_by(date_asdate) %>%
                                dplyr::summarise(resid_perc_mean=mean(resid_perc, na.rm=T),
                                          resid_perc_50=quantile(resid_perc, probs=.50,na.rm=T),
                                          resid_perc_90=quantile(resid_perc, probs=.90,na.rm=T),
                                          resid_perc_99=quantile(resid_perc, probs=.99,na.rm=T),
                                          )

#Ok this is percent difference over time, shows disagreement in the middle
#more helpful if we put this in terms of t since first confirmed or 100 confirmed
lhs_long_clean_pairwise_date %>% 
                    ggplot(aes(date_asdate)) + 
                    geom_line(aes(y=resid_perc_50)) + 
                    geom_line(aes(y=resid_perc_90)) + 
                    geom_line(aes(y=resid_perc_99)) +
                    theme_bw() + ylab("Percent Difference Across Sources")
```

```{r}
#Percent difference by datasource
lhs_long_clean_pairwise_source <- lhs_long_clean_pairwise %>%
                                  group_by(dataset.x) %>%
                                  dplyr::summarise(resid_perc_mean=mean(resid_perc, na.rm=T) %>% round(2),
                                            resid_perc_50=quantile(resid_perc, probs=.50,na.rm=T) %>% round(2),
                                            resid_perc_90=quantile(resid_perc, probs=.90,na.rm=T) %>% round(2),
                                            resid_perc_99=quantile(resid_perc, probs=.99,na.rm=T) %>% round(2),
                                            n=n()
                                            )

```

```{r, results="show"}
lhs_long_clean_pairwise_source %>% arrange(-resid_perc_mean) %>% DT::datatable(options = list(pageLength = 20, autoWidth = TRUE))
```

```{r}
#By location
lhs_long_clean_pairwise_location <- lhs_long_clean_pairwise %>%
                                    group_by(gid_geonameid_wikidata_id) %>%
                                    dplyr::summarise(resid_perc_mean=mean(resid_perc, na.rm=T) %>% round(2),
                                              resid_perc_50=quantile(resid_perc, probs=.50,na.rm=T) %>% round(2),
                                              resid_perc_90=quantile(resid_perc, probs=.90,na.rm=T) %>% round(2),
                                              resid_perc_99=quantile(resid_perc, probs=.99,na.rm=T) %>% round(2),
                                              n=n()
                                              )
```
                                            
```{r, results="show"}
lhs_long_clean_pairwise_location %>% arrange(-resid_perc_mean) %>% DT::datatable(options = list(pageLength = 20, autoWidth = TRUE))
```



## Takeaways

Any data aggregation and cleaning approach will have to deal with the following issues

* Missingness
    + Prior to the first reported observation
    + After the last reported observation
    + Within a time series between observations
    + Unbalanced across different sources
* Structural Changes
    + Changes in reporting criteria/definitions
    + Changes in sourcing for unerlying data
* Disagreement
    + One or more sources report different numbers
* Errors
    + Outliers
    + Merging errors
* Bias
    + Correlation between missingness and measurement
    + Attenuation bias
    


<!--chapter:end:02-data.Rmd-->


```{r,warning=FALSE,message=FALSE,error=FALSE, results='hide' , include=FALSE}
knitr::opts_chunk$set( out.width='100%', fig.asp=NULL, fig.align='center', echo=F, warning=FALSE,message=FALSE,error=FALSE, results='hide', cache=T  ) 

#Notes to self
#It's not like R notebook you have the knit every time you want to update html, you can't just save
#You don't have to rerun the code or rebuild the whole book though.

```

```{r}
#Library Loads
library(scales) 
library(tidyverse)
# stable version on CRAN
#install.packages("bookdown")
# or development version on GitHub
# devtools::install_github('rstudio/bookdown')
#libraries
library(lubridate)
library(tidyverse)

#devtools::install_github("ropensci/USAboundaries")
#devtools::install_github("ropensci/USAboundariesData")
library(USAboundaries) ; #install.packages('USAboundaries')
data(state_codes)

library(tidyverse)
library(scales)
library(gghighlight)
library(lubridate)
library(R0)  # consider moving all library commands to top -- this one was in a loop below

library(WikidataR)
library(countrycode)

library(usmap) ; # install.packages('usmap')
data(statepop)
#devtools::install_github("ropensci/USAboundaries")
#devtools::install_github("ropensci/USAboundariesData")
library(USAboundaries) ; #install.packages('USAboundaries')
data(state_codes)

library(tidyverse)
library(sf)

library(jsonlite)

#This is too slow it's downloading each
library(GADMTools)
library(strucchange) ; #install.packages('strucchange')
library(tsibble)

library(patchwork)
```

# Tests

## Tested People versus Tested Samples

## Interpolate Within Observed

## Interplate Prior to Observed

## Interpolate Subnationally

## Explaining Variation in Testing



South Korea

Vietnam
https://www.reuters.com/article/us-health-coronavirus-vietnam-fight-insi-idUSKBN22B34H




<!--chapter:end:03-tests.Rmd-->

# Common Measures of Interest


## R0 and R

## Case Fatality Rate (CFR)

## Percent Positive

<!--chapter:end:04-confirmed.Rmd-->

# Final Words

We have finished a nice book.

<!--chapter:end:05-deaths.Rmd-->


# Actual Infections

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

<!--chapter:end:06-actual_infections.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`

<!--chapter:end:07-references.Rmd-->

