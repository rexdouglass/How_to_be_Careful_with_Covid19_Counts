mutate(confirmed_log = log(confirmed)) %>%
mutate(deaths_log = log(deaths)) %>%
mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>%
filter(!is.na(confirmed_log) ) %>%
filter(wikidata_id=="Q500416") %>%
arrange(date_asdate)
p_over_time_by_source_Manhattan <- test %>%
ggplot() +
geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
theme_bw() +
facet_wrap(~dataset   ) +
ylab("Cumulative Count (log)") + xlab("Date") +
ggtitle("Manhattan (Q500416) Counts")
#Staten Island is richmond county Q11997784
test <- lhs_long_clean %>%
mutate(confirmed_log = log(confirmed)) %>%
mutate(deaths_log = log(deaths)) %>%
mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>%
filter(!is.na(confirmed_log) ) %>%
filter(wikidata_id=="Q11997784") %>%
arrange(date_asdate)
p_over_time_by_source_Staten_Island <- test %>%
ggplot() +
geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
theme_bw() +
facet_wrap(~dataset   ) +
ylab("Cumulative Count (log)") + xlab("Date") +
ggtitle("Staten Island  (Q11997784) Counts")
p_over_time_by_source_Bronx +
#p_over_time_by_source_Brooklyn + #I need to fix brooklyn
p_over_time_by_source_Queens +
p_over_time_by_source_Manhattan +
p_over_time_by_source_Staten_Island
temp <- lhs_long_clean %>%
filter(!is.na(wikidata_id)) %>% #the NA_NA_NA obs were screwing with us
dplyr::select(dataset, gid_geonameid_wikidata_id, date_asdate, confirmed) %>%
filter(!is.na(gid_geonameid_wikidata_id) & !is.na(date_asdate)) %>%
filter(!is.na(confirmed) & confirmed>0 )
lhs_long_clean_pairwise <- temp %>% left_join(temp, by=c('gid_geonameid_wikidata_id', 'date_asdate'))
dim(lhs_long_clean_pairwise) #3,304,496
lhs_long_clean_pairwise <- lhs_long_clean_pairwise %>%
filter(dataset.x!=dataset.y) %>%
mutate(resid_perc= abs((confirmed.x/confirmed.y)-1)  )
dim(lhs_long_clean_pairwise) #2,673,952
lhs_long_clean_pairwise$resid_perc %>% summary() #About within 2% on average but exact usually
lhs_long_clean_pairwise_date <- lhs_long_clean_pairwise %>%
group_by(date_asdate) %>%
dplyr::summarise(resid_perc_mean=mean(resid_perc, na.rm=T),
resid_perc_50=quantile(resid_perc, probs=.50,na.rm=T),
resid_perc_90=quantile(resid_perc, probs=.90,na.rm=T),
resid_perc_99=quantile(resid_perc, probs=.99,na.rm=T),
)
#Ok this is percent difference over time, shows disagreement in the middle
#more helpful if we put this in terms of t since first confirmed or 100 confirmed
lhs_long_clean_pairwise_date %>%
ggplot(aes(date_asdate)) +
geom_line(aes(y=resid_perc_50)) +
geom_line(aes(y=resid_perc_90)) +
geom_line(aes(y=resid_perc_99)) +
theme_bw() + ylab("Percent Difference Across Sources")
#Percent difference by datasource
lhs_long_clean_pairwise_source <- lhs_long_clean_pairwise %>%
group_by(dataset.x) %>%
dplyr::summarise(resid_perc_mean=mean(resid_perc, na.rm=T) %>% round(2),
resid_perc_50=quantile(resid_perc, probs=.50,na.rm=T) %>% round(2),
resid_perc_90=quantile(resid_perc, probs=.90,na.rm=T) %>% round(2),
resid_perc_99=quantile(resid_perc, probs=.99,na.rm=T) %>% round(2),
n=n()
)
lhs_long_clean_pairwise_source %>% arrange(-resid_perc_mean) %>% DT::datatable(options = list(pageLength = 20, autoWidth = TRUE))
#By location
lhs_long_clean_pairwise_location <- lhs_long_clean_pairwise %>%
group_by(gid_geonameid_wikidata_id) %>%
dplyr::summarise(resid_perc_mean=mean(resid_perc, na.rm=T) %>% round(2),
resid_perc_50=quantile(resid_perc, probs=.50,na.rm=T) %>% round(2),
resid_perc_90=quantile(resid_perc, probs=.90,na.rm=T) %>% round(2),
resid_perc_99=quantile(resid_perc, probs=.99,na.rm=T) %>% round(2),
n=n()
)
lhs_long_clean_pairwise_location %>% arrange(-resid_perc_mean) %>% DT::datatable(options = list(pageLength = 20, autoWidth = TRUE))
test <- lhs_long_clean %>%
mutate(confirmed_log = log(confirmed)) %>%
mutate(deaths_log = log(deaths)) %>%
mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>%
filter(!is.na(confirmed_log) ) %>%
filter(wikidata_id=="Q30") %>%
arrange(date_asdate)
View(test)
knitr::opts_chunk$set( out.width='100%', fig.asp=NULL, fig.align='center', echo=F, warning=FALSE,message=FALSE,error=FALSE, results='hide', cache=T  )
#Notes to self
#It's not like R notebook you have the knit every time you want to update html, you can't just save
#You don't have to rerun the code or rebuild the whole book though.
#Library Loads
library(scales)
library(tidyverse)
# stable version on CRAN
#install.packages("bookdown")
# or development version on GitHub
# devtools::install_github('rstudio/bookdown')
#libraries
library(lubridate)
library(tidyverse)
#devtools::install_github("ropensci/USAboundaries")
#devtools::install_github("ropensci/USAboundariesData")
library(USAboundaries) ; #install.packages('USAboundaries')
data(state_codes)
library(tidyverse)
library(scales)
library(gghighlight)
library(lubridate)
library(R0)  # consider moving all library commands to top -- this one was in a loop below
library(WikidataR)
library(countrycode)
library(usmap) ; # install.packages('usmap')
data(statepop)
#devtools::install_github("ropensci/USAboundaries")
#devtools::install_github("ropensci/USAboundariesData")
library(USAboundaries) ; #install.packages('USAboundaries')
data(state_codes)
library(tidyverse)
library(sf)
library(jsonlite)
#This is too slow it's downloading each
library(GADMTools)
library(strucchange) ; #install.packages('strucchange')
library(tsibble)
library(patchwork)
library(DT)
#Data Loads
lhs_long <- readRDS("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/lhs_long.Rds")
#Data loads
lhs_long_clean <- readRDS( "/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/lhs_long_cleand.Rds")
#dim(lhs_long) #187,305
#lhs_long_clean$dataset
print("Number of observations")
dim(lhs_long_clean)
print("Number of locations")
lhs_long_clean$wikidata_id %>% unique() %>% length() #3,373
print("Number of Days")
lhs_long_clean$date_asdate %>% unique() %>% length() #113
library(DT) #https://rstudio.github.io/DT/
#library(gt)
#lhs_long_clean %>% dplyr::count(dataset) %>% arrange(-n) %>% gt() %>%
# tab_header(
#    title = md("Location-Days by Dataset")#,
#    #subtitle = "Number of Days with Data by Dataset"
#  ) %>%
#  fmt_missing( columns=everything(), rows=NULL,missing_text = 0)
lhs_long_clean %>% dplyr::count(dataset) %>% arrange(-n) %>% DT::datatable(options = list(pageLength = 20, autoWidth = TRUE))
#There's a miscode of New York City and New York state that creates an issue
lhs_long_clean_confirmed_wide <-
lhs_long_clean %>%
filter(!is.na(wikidata_id)) %>% #the NA_NA_NA obs were screwing with us
dplyr::select(dataset, gid_geonameid_wikidata_id, date_asdate, confirmed) %>%
filter(!is.na(gid_geonameid_wikidata_id) & !is.na(date_asdate)) %>%
filter(!is.na(confirmed) & confirmed>0 ) %>%
group_by(dataset, gid_geonameid_wikidata_id, date_asdate) %>%
dplyr::summarise(confirmed= max(confirmed, na.rm=T)) %>%
ungroup() %>%
pivot_wider(names_from = dataset, values_from = confirmed ) %>%
mutate_if(is.numeric, list(~na_if(abs(.), Inf))) %>% distinct()
dim(lhs_long_clean_confirmed_wide) #154,499 154k location days
temp <- lhs_long_clean_confirmed_wide  %>% mutate_if(is.numeric, replace_na, 0) %>% mutate_if(is.numeric, ~  as.numeric(.>0 )) #%>% dplyr::select(-gid_geonameid_wikidata_id, -date_asdate)
library(UpSetR) #install.packages('UpSetR')
upset(temp %>% as.data.frame(), nsets = 25, nintersects = 30, mb.ratio = c(0.4, 0.6),
order.by = c("freq", "degree"), decreasing = c(T,FALSE))
detach(package:UpSetR, unload=TRUE) #I currently suspect this of breaking dplyr in really subtle quite ways that are noooot good.
lhs_long_place_sources <- lhs_long  %>% dplyr::select(gid,   geonameid, wikidata_id, dataset) %>%
group_by(gid,  geonameid, wikidata_id) %>% dplyr::count(dataset) %>%
group_by(gid,  geonameid, wikidata_id) %>%
dplyr::summarise(datasets_n=n())
lhs_long_place_sources_testing <- lhs_long  %>%
dplyr::select(gid,  geonameid, wikidata_id, date_asdate, confirmed, tested_people, tested_samples) %>%
group_by(gid,  geonameid, wikidata_id, date_asdate) %>%
mutate_if(is.numeric, max, na.rm=T) %>% #this takes a max across all datasets for that day
filter(!duplicated(date_asdate)) %>%
ungroup() %>%
mutate_if(is.numeric, list(~na_if(., -Inf))) %>%
mutate_if(is.numeric, list(~na_if(., Inf))) %>%
mutate(percent_of_days_without_testing=!is.na(date_asdate) & (is.na(tested_people) & is.na(tested_samples)  )) %>%
group_by(gid,  geonameid, wikidata_id) %>%
dplyr::summarise(
percent_of_days_without_testing=sum(percent_of_days_without_testing, na.rm=T)/n()
) %>%
mutate(percent_of_days_with_testing= 1-percent_of_days_without_testing)
#gadm36 = st_read("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESSgadm/data_in/gadm36_gpkg/gadm36.gpkg")
#st_layers("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/gadm36_bycountry/gadm36_levels_gpkg/gadm36_levels.gpkg")
gadm36_levels_0 = st_read("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/gadm36_bycountry/gadm36_levels_gpkg/gadm36_levels.gpkg", layer="level0")  %>%
st_simplify(preserveTopology = FALSE, dTolerance =0.1) #  0.025 this is supposedly broken up by 6 levels and so should have u.s.
#plot(gadm36_levels_0)
#dim(gadm36_levels_0_sf$sf)
#gadm_plot(gadm36_levels_0_sf)
p_datasets_by_country <- gadm36_levels_0 %>%
left_join(lhs_long_place_sources %>% dplyr::select(gid=gid, datasets_n) %>%  left_join( gadm36_levels_0 %>% as.data.frame() %>% dplyr::select(gid=GID_0, NAME_0)  %>% distinct()  )
) %>%
#replace_na(list(datasets_n = 0)) %>%
ggplot() + geom_sf(aes(fill = datasets_n)) +
scale_fill_gradient(name= "# Databases", low="red", high="green") +
theme_bw() + ggtitle("Number of Databases with Coverage of each Country")
p0_testing <- gadm36_levels_0 %>%
left_join(lhs_long_place_sources_testing %>% dplyr::select(gid=gid, percent_of_days_with_testing) %>%  left_join( gadm36_levels_0 %>% as.data.frame() %>% dplyr::select(gid=GID_0, NAME_0)  %>% distinct()  )
) %>%
#replace_na(list(datasets_n = 0)) %>%
ggplot() + geom_sf(aes(fill = percent_of_days_with_testing)) +
scale_fill_gradient(name= "Percent", low="red", high="green") +
theme_bw() + ggtitle("Percent of Days with Testing Counts")
p_datasets_by_country / p0_testing
gadm36_levels_1 = st_read("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/gadm36_bycountry/gadm36_levels_gpkg/gadm36_levels.gpkg", layer="level1")  %>%
st_simplify(preserveTopology = FALSE, dTolerance =0.1) #  0.025 this is supposedly broken up by 6 levels and so should have u.s.
p_datasets_by_state <- gadm36_levels_1 %>%
left_join(lhs_long_place_sources %>% dplyr::select(gid=gid, datasets_n) %>%  left_join( gadm36_levels_1 %>% as.data.frame() %>% dplyr::select(gid=GID_1, NAME_1) %>% distinct()  )
) %>%
#replace_na(list(datasets_n = 0)) %>%
ggplot() + geom_sf(aes(fill = datasets_n)) +
scale_fill_gradient(name= "# Databases", low="red", high="green") +
theme_bw()  + ggtitle("Number of Databases with Coverage of each State/Province")
p1_testing <- gadm36_levels_1 %>%
left_join(lhs_long_place_sources_testing %>% dplyr::select(gid=gid, percent_of_days_with_testing) %>%  left_join( gadm36_levels_1 %>% as.data.frame() %>% dplyr::select(gid=GID_1, NAME_1) %>% distinct()  )
) %>%
#replace_na(list(datasets_n = 0)) %>%
ggplot() + geom_sf(aes(fill = percent_of_days_with_testing)) +
scale_fill_gradient(name= "Percent", low="red", high="green") +
theme_bw()  + ggtitle("Percent of Days with Testing Counts")
p_datasets_by_state/p1_testing
gadm36_levels_2 = st_read("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/gadm36_bycountry/gadm36_levels_gpkg/gadm36_levels.gpkg", layer="level2")  %>%
st_simplify(preserveTopology = FALSE, dTolerance = 0.001) #  0.025 this is supposedly broken up by 6 levels and so should have u.s. I have to keep shrinking it so misisng goes to zero
#China
#"Q148"
test <- lhs_long_clean %>%
mutate(confirmed_log = log(confirmed)) %>%
mutate(deaths_log = log(deaths)) %>%
mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>%
filter(!is.na(confirmed_log) ) %>%
filter(wikidata_id=="Q148") %>%
arrange(date_asdate) #>%
p_over_time_by_source_china <- test %>%
ggplot() +
geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
theme_bw() +
facet_wrap(~dataset   ) +
ylab("Cumulative Count (log)") + xlab("Date") +
ggtitle("China Q148 Counts")
p_over_time_by_source_china
#Italy
#"Q38"
test <- lhs_long_clean %>%
mutate(confirmed_log = log(confirmed)) %>%
mutate(deaths_log = log(deaths)) %>%
mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>%
filter(!is.na(confirmed_log) ) %>%
filter(wikidata_id=="Q38") %>%
arrange(date_asdate) #%>%
#mutate( confirmed_log_y_hat_percent_change = breakpoints(formula=confirmed_log_y_hat_percent_change ~ 1, data=. ) %>% fitted.values() )
p_over_time_by_source_italy <- test %>%
ggplot() +
geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
theme_bw() +
ylab("Cumulative Count (log)") + xlab("Date") +
ggtitle("Italy Q38 Counts") +
facet_wrap(~dataset   )
p_over_time_by_source_italy
#U.S.
#"Q30"
test <- lhs_long_clean %>%
mutate(confirmed_log = log(confirmed)) %>%
mutate(deaths_log = log(deaths)) %>%
mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>%
filter(!is.na(confirmed_log) ) %>%
filter(wikidata_id=="Q30") %>%
arrange(date_asdate)
p_over_time_by_source_us <- test %>%
ggplot() +
geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
theme_bw() +
facet_wrap(~dataset   ) +
ylab("Cumulative Count (log)") + xlab("Date") +
ggtitle("United States Q30 Counts")
p_over_time_by_source_us
#New York State
#"Q30"
test <- lhs_long_clean %>%
mutate(confirmed_log = log(confirmed)) %>%
mutate(deaths_log = log(deaths)) %>%
mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>%
filter(!is.na(confirmed_log) ) %>%
filter(wikidata_id=="Q1384") %>%
arrange(date_asdate)
p_over_time_by_source_new_york_state <- test %>%
ggplot() +
geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
theme_bw() +
facet_wrap(~dataset   ) +
ylab("Cumulative Count (log)") + xlab("Date") +
ggtitle("New York State Q1384 Counts")
p_over_time_by_source_new_york_state
#Bronx County (Q855974)
test <- lhs_long_clean %>%
mutate(confirmed_log = log(confirmed)) %>%
mutate(deaths_log = log(deaths)) %>%
mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>%
filter(!is.na(confirmed_log) ) %>%
filter(wikidata_id=="Q855974") %>%
arrange(date_asdate)
p_over_time_by_source_Bronx <- test %>%
ggplot() +
geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
theme_bw() +
facet_wrap(~dataset   ) +
ylab("Cumulative Count (log)") + xlab("Date") +
ggtitle("Bronx County (Q855974) Counts")
#Brooklyn (Q18419)
test <- lhs_long_clean %>%
mutate(confirmed_log = log(confirmed)) %>%
mutate(deaths_log = log(deaths)) %>%
mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>%
filter(!is.na(confirmed_log) ) %>%
filter(wikidata_id=="Q18419") %>%
arrange(date_asdate)
p_over_time_by_source_Brooklyn <- test %>%
ggplot() +
geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
theme_bw() +
facet_wrap(~dataset   ) +
ylab("Cumulative Count (log)") + xlab("Date") +
ggtitle("Brooklyn County (Q18419) Counts")
#Queens County, New York Q5142559
test <- lhs_long_clean %>%
mutate(confirmed_log = log(confirmed)) %>%
mutate(deaths_log = log(deaths)) %>%
mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>%
filter(!is.na(confirmed_log) ) %>%
filter(wikidata_id=="Q5142559") %>%
arrange(date_asdate)
p_over_time_by_source_Queens <- test %>%
ggplot() +
geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
theme_bw() +
facet_wrap(~dataset   ) +
ylab("Cumulative Count (log)") + xlab("Date") +
ggtitle("Queens County (Q18419) Counts")
#Manhattan is New York County Q500416
test <- lhs_long_clean %>%
mutate(confirmed_log = log(confirmed)) %>%
mutate(deaths_log = log(deaths)) %>%
mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>%
filter(!is.na(confirmed_log) ) %>%
filter(wikidata_id=="Q500416") %>%
arrange(date_asdate)
p_over_time_by_source_Manhattan <- test %>%
ggplot() +
geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
theme_bw() +
facet_wrap(~dataset   ) +
ylab("Cumulative Count (log)") + xlab("Date") +
ggtitle("Manhattan (Q500416) Counts")
#Staten Island is richmond county Q11997784
test <- lhs_long_clean %>%
mutate(confirmed_log = log(confirmed)) %>%
mutate(deaths_log = log(deaths)) %>%
mutate(tested_log = log(coalesce(tested_people, tested_samples) )) %>%
filter(!is.na(confirmed_log) ) %>%
filter(wikidata_id=="Q11997784") %>%
arrange(date_asdate)
p_over_time_by_source_Staten_Island <- test %>%
ggplot() +
geom_point( aes(x=date_asdate, y=tested_log), alpha=1, color="blue", size=.5) +
geom_point( aes(x=date_asdate, y=confirmed_log), alpha=1, color="red", size=.5) +
geom_point( aes(x=date_asdate, y=deaths_log), alpha=1, color="black", size=.5) +
theme_bw() +
facet_wrap(~dataset   ) +
ylab("Cumulative Count (log)") + xlab("Date") +
ggtitle("Staten Island  (Q11997784) Counts")
p_over_time_by_source_Bronx +
#p_over_time_by_source_Brooklyn + #I need to fix brooklyn
p_over_time_by_source_Queens +
p_over_time_by_source_Manhattan +
p_over_time_by_source_Staten_Island
temp <- lhs_long_clean %>%
filter(!is.na(wikidata_id)) %>% #the NA_NA_NA obs were screwing with us
dplyr::select(dataset, gid_geonameid_wikidata_id, date_asdate, confirmed) %>%
filter(!is.na(gid_geonameid_wikidata_id) & !is.na(date_asdate)) %>%
filter(!is.na(confirmed) & confirmed>0 )
lhs_long_clean_pairwise <- temp %>% left_join(temp, by=c('gid_geonameid_wikidata_id', 'date_asdate'))
dim(lhs_long_clean_pairwise) #3,304,496
lhs_long_clean_pairwise <- lhs_long_clean_pairwise %>%
filter(dataset.x!=dataset.y) %>%
mutate(resid_perc= abs((confirmed.x/confirmed.y)-1)  )
dim(lhs_long_clean_pairwise) #2,673,952
lhs_long_clean_pairwise$resid_perc %>% summary() #About within 2% on average but exact usually
lhs_long_clean_pairwise_date <- lhs_long_clean_pairwise %>%
group_by(date_asdate) %>%
dplyr::summarise(resid_perc_mean=mean(resid_perc, na.rm=T),
resid_perc_50=quantile(resid_perc, probs=.50,na.rm=T),
resid_perc_90=quantile(resid_perc, probs=.90,na.rm=T),
resid_perc_99=quantile(resid_perc, probs=.99,na.rm=T),
)
#Ok this is percent difference over time, shows disagreement in the middle
#more helpful if we put this in terms of t since first confirmed or 100 confirmed
lhs_long_clean_pairwise_date %>%
ggplot(aes(date_asdate)) +
geom_line(aes(y=resid_perc_50)) +
geom_line(aes(y=resid_perc_90)) +
geom_line(aes(y=resid_perc_99)) +
theme_bw() + ylab("Percent Difference Across Sources")
#Percent difference by datasource
lhs_long_clean_pairwise_source <- lhs_long_clean_pairwise %>%
group_by(dataset.x) %>%
dplyr::summarise(resid_perc_mean=mean(resid_perc, na.rm=T) %>% round(2),
resid_perc_50=quantile(resid_perc, probs=.50,na.rm=T) %>% round(2),
resid_perc_90=quantile(resid_perc, probs=.90,na.rm=T) %>% round(2),
resid_perc_99=quantile(resid_perc, probs=.99,na.rm=T) %>% round(2),
n=n()
)
lhs_long_clean_pairwise_source %>% arrange(-resid_perc_mean) %>% DT::datatable(options = list(pageLength = 20, autoWidth = TRUE))
#By location
lhs_long_clean_pairwise_location <- lhs_long_clean_pairwise %>%
group_by(gid_geonameid_wikidata_id) %>%
dplyr::summarise(resid_perc_mean=mean(resid_perc, na.rm=T) %>% round(2),
resid_perc_50=quantile(resid_perc, probs=.50,na.rm=T) %>% round(2),
resid_perc_90=quantile(resid_perc, probs=.90,na.rm=T) %>% round(2),
resid_perc_99=quantile(resid_perc, probs=.99,na.rm=T) %>% round(2),
n=n()
)
lhs_long_clean_pairwise_location %>% arrange(-resid_perc_mean) %>% DT::datatable(options = list(pageLength = 20, autoWidth = TRUE))
knitr::opts_chunk$set( out.width='100%', fig.asp=NULL, fig.align='center', echo=F, warning=FALSE,message=FALSE,error=FALSE, results='hide', cache=T  )
#Notes to self
#It's not like R notebook you have the knit every time you want to update html, you can't just save
#You don't have to rerun the code or rebuild the whole book though.
#Library Loads
library(scales)
library(tidyverse)
# stable version on CRAN
#install.packages("bookdown")
# or development version on GitHub
# devtools::install_github('rstudio/bookdown')
library(tsibble)
lhs_long <- readRDS("/media/skynet2/905884f0-7546-4273-9061-12a790830beb/rwd_github_private/NESScovid19/data_temp/lhs_long.Rds")
dim(lhs_long) #779818
####crap, some package I loaded breaks dplyr badly. The only one I can think of is UpSetR.
lhs_long_clean <- lhs_long %>% as.data.frame() %>% ungroup() %>%
#filter(wikidata_id=="Q30" ) %>% #& dataset=="yahoo"
mutate(dataset_gid_geonameid_wikidata_id= paste(dataset,gid,geonameid, wikidata_id, sep="_")  ) %>%
mutate(gid_geonameid_wikidata_id= paste(gid,geonameid, wikidata_id, sep="_")  ) %>%
dplyr::select(dataset_gid_geonameid_wikidata_id, gid_geonameid_wikidata_id, dataset, gid,geonameid, wikidata_id ,date_asdate, confirmed, deaths, tested_people, tested_samples) %>%
group_by(dataset_gid_geonameid_wikidata_id ,date_asdate) %>% #if the same source has multiple values on the same thing on the same day, or different values across unmerged obs, then take the max
summarise_all(max, na.rm=T) %>%
ungroup() %>%
mutate_if(is.numeric, list(~na_if(., -Inf))) %>%
mutate_if(is.numeric, list(~na_if(., Inf))) %>%
filter(!is.na(date_asdate)) %>%
#First drop anything that's negative. Shouldn't be any negatives.
filter(is.na(confirmed) | confirmed>=0) %>%
filter(is.na(deaths) | deaths>=0) %>%
filter(is.na(tested_people) | tested_people>=0) %>%
filter(is.na(tested_samples) | tested_samples>=0) %>%
#Then set 0s to NA, we don't trust NAs
mutate(confirmed=ifelse(confirmed==0, NA, confirmed)) %>%
mutate(deaths=ifelse(deaths==0, NA, deaths)) %>%
mutate(tested_people=ifelse(tested_people==0, NA, tested_people)) %>%
mutate(tested_samples=ifelse(tested_samples==0, NA, tested_samples)) %>%
#Then check first differences and drop anything that doesn't weakly increase monotonically
dplyr::group_by(  dataset_gid_geonameid_wikidata_id ) %>%  #it's not grouping correctly all of a sudden?
dplyr::arrange(date_asdate) %>%
mutate(confirmed_cummax =    confirmed %>% replace_na(0) %>% cummax(),
deaths_cummax    =    deaths %>% replace_na(0) %>% cummax(),
tested_people_cummax= tested_people %>% replace_na(0) %>% cummax(),
tested_samples_cummax=tested_samples %>% replace_na(0) %>% cummax()
) %>%
ungroup() %>%
#About 4k of these observations show a decrease from one time step to the next which suggests either an original error or a joining error
#We're going to straight drop those observations as a cleaning step
#this isn't enough because if you have consecutive mistakes it'll show a positive fd even if it's not good enough
filter( (is.na(confirmed) | confirmed>=confirmed_cummax) &  #never allow for a reversal
(is.na(deaths) | deaths>=deaths_cummax) &        #never allow for a reversal
(is.na(tested_people) | tested_people>=tested_people_cummax) &  #never allow for a reversal
(is.na(tested_samples) | tested_samples>=tested_samples_cummax)  #never allow for a reversal
) %>%
#we're going to go one step further and require either confirmed or tested to be strictly higher
#In words, during an episode we don't believe reports mean "no new cases" just no new good reporting
#metabiota is the worst offender here so restricting it to just that
filter( !(
dataset=="metabiota" & #is metabiota
(!is.na(confirmed) &   #with a non missing confirmed
confirmed<=confirmed_cummax ) #that is equal to or less than the cumulative max until then.
) #reject these
) %>% #metabiota is really problematic 90% of what we're doing is trying to account for it
#Also reject any where deaths are greater than confirmed
filter(is.na(confirmed) | is.na(deaths) | confirmed>=deaths) %>%
#also reject any where confirmed doesn't vary at all at that location
group_by( gid_geonameid_wikidata_id) %>% #
filter( confirmed %>% unique() %>% length() > 1 ) %>%
ungroup()
